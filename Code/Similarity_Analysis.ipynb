{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Final.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storyDf = pd.DataFrame()\n",
    "storyDf['story_urlA'] = df['story_urlA'].values\n",
    "storyDf['story_urlB'] = df['story_urlB'].values\n",
    "storyDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text lowercase\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# remove numbers\n",
    "def remove_numbers(text):\n",
    "    result = re.sub(r'\\d+', '', text)\n",
    "    return result\n",
    "\n",
    "# remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "# remove whitespace from text\n",
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())\n",
    "\n",
    "# remove stopwords function\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "input_str = storyDf.story_urlA[0]\n",
    "print(input_str)\n",
    "\n",
    "text_lowercase(input_str)\n",
    "remove_numbers(input_str)\n",
    "remove_punctuation(input_str)\n",
    "remove_whitespace(input_str)\n",
    "remove_stopwords(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "cos_sim = dot(storyDf['story_urlA'], storyDf['story_urlB'])/(norm(storyDf['story_urlA'])*norm(storyDf['story_urlB']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# cosine_similarity(storyDf['story_urlA'].values.reshape(1, -1), storyDf['story_urlB'].values.reshape(1, -1))\n",
    "cosine_similarity(storyDf['story_urlA'].iloc[3], storyDf['story_urlB'].iloc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Import your data to a Pandas.DataFrame\n",
    "df = pd.read_csv('Final.csv')\n",
    "\n",
    "# Grab the column you'd like to group, filter out duplicate values\n",
    "# and make sure the values are Unicode\n",
    "vals = df['story_urlA'].unique().astype('U')\n",
    "\n",
    "\n",
    "# Write a function for cleaning strings and returning an array of ngrams\n",
    "def ngrams_analyzer(string):\n",
    "    string = re.sub(r'[,-./]', r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(5)])  # N-Gram length is 5\n",
    "    return [''.join(ngram) for ngram in ngrams]\n",
    "\n",
    "# Construct your vectorizer for building the TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(analyzer=ngrams_analyzer)\n",
    "\n",
    "# Build the matrix!!!\n",
    "tfidf_matrix = vectorizer.fit_transform(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import IGN's awesome_cossim_topn module\n",
    "from sparse_dot_topn import awesome_cossim_topn\n",
    "\n",
    "# The arguments for awesome_cossim_topn are as follows:\n",
    "### 1. Our TF-IDF matrix\n",
    "### 2. Our TF-IDF matrix transposed (allowing us to build a pairwise cosine matrix)\n",
    "### 3. A top_n filter, which allows us to filter the number of matches returned, which isn't useful for our purposes\n",
    "### 4. This is our similarity threshold. Only values over 0.8 will be returned\n",
    "cosine_matrix = awesome_cossim_topn(\n",
    "  tfidf_matrix,\n",
    "  tfidf_matrix.transpose(),\n",
    "  vals.size,\n",
    "  0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sparse_dot_topn import awesome_cossim_topn\n",
    "\n",
    "# Import your data to a Pandas.DataFrame\n",
    "df = pd.read_csv('Final.csv')\n",
    "\n",
    "# Instaniate our lookup hash table\n",
    "group_lookup = {}\n",
    "\n",
    "\n",
    "# Write a function for cleaning strings and returning an array of ngrams\n",
    "def ngrams_analyzer(string):\n",
    "    string = re.sub(r'[,-./]', r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(5)])  # N-Gram length is 5\n",
    "    return [''.join(ngram) for ngram in ngrams]\n",
    "\n",
    "\n",
    "def find_group(row, col):\n",
    "    # If either the row or the col string have already been given\n",
    "    # a group, return that group. Otherwise return none\n",
    "    if row in group_lookup:\n",
    "        return group_lookup[row]\n",
    "    elif col in group_lookup:\n",
    "        return group_lookup[col]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def add_vals_to_lookup(group, row, col):\n",
    "    # Once we know the group name, set it as the value\n",
    "    # for both strings in the group_lookup\n",
    "    group_lookup[row] = group\n",
    "    group_lookup[col] = group\n",
    "\n",
    "\n",
    "def add_pair_to_lookup(row, col):\n",
    "    # in this function we'll add both the row and the col to the lookup\n",
    "    group = find_group(row, col)  # first, see if one has already been added\n",
    "    if group is not None:\n",
    "        # if we already know the group, make sure both row and col are in lookup\n",
    "        add_vals_to_lookup(group, row, col)\n",
    "    else:\n",
    "        # if we get here, we need to add a new group.\n",
    "        # The name is arbitrary, so just make it the row\n",
    "        add_vals_to_lookup(row, row, col)\n",
    "\n",
    "\n",
    "# Construct your vectorizer for building the TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(analyzer=ngrams_analyzer)\n",
    "\n",
    "# Grab the column you'd like to group, filter out duplicate values\n",
    "# and make sure the values are Unicode\n",
    "vals = df['story_urlA'].unique().astype('U')\n",
    "\n",
    "# Build the matrix!!!\n",
    "tfidf_matrix = vectorizer.fit_transform(vals)\n",
    "\n",
    "cosine_matrix = awesome_cossim_topn(tfidf_matrix, tfidf_matrix.transpose(), vals.size, 0.8)\n",
    "\n",
    "# Build a coordinate matrix\n",
    "coo_matrix = cosine_matrix.tocoo()\n",
    "\n",
    "# for each row and column in coo_matrix\n",
    "# if they're not the same string add them to the group lookup\n",
    "for row, col in zip(coo_matrix.row, coo_matrix.col):\n",
    "    if row != col:\n",
    "        add_pair_to_lookup(vals[row], vals[col])\n",
    "\n",
    "df['Group'] = df['story_urlA'].map(group_lookup).fillna(df['story_urlA'])\n",
    "\n",
    "df.to_csv('data-grouped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program to measure the similarity between \n",
    "# two sentences using cosine similarity.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df = pd.read_csv('Final.csv')\n",
    "\n",
    "# X = input(\"Enter first string: \").lower()\n",
    "# Y = input(\"Enter second string: \").lower()\n",
    "X = df['story_urlA'].iloc[0]\n",
    "Y = df['story_urlB'].iloc[0]\n",
    "  \n",
    "# tokenization\n",
    "X_list = word_tokenize(X) \n",
    "Y_list = word_tokenize(Y)\n",
    "  \n",
    "# sw contains the list of stopwords\n",
    "sw = stopwords.words('english') \n",
    "l1 =[];l2 =[]\n",
    "  \n",
    "# remove stop words from the string\n",
    "X_set = {w for w in X_list if not w in sw} \n",
    "Y_set = {w for w in Y_list if not w in sw}\n",
    "  \n",
    "# form a set containing keywords of both strings \n",
    "rvector = X_set.union(Y_set) \n",
    "for w in rvector:\n",
    "    if w in X_set: l1.append(1) # create a vector\n",
    "    else: l1.append(0)\n",
    "    if w in Y_set: l2.append(1)\n",
    "    else: l2.append(0)\n",
    "c = 0\n",
    "  \n",
    "# cosine formula \n",
    "for i in range(len(rvector)):\n",
    "        c+= l1[i]*l2[i]\n",
    "cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "print(\"similarity: \", cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_Sim(X, Y):\n",
    "\n",
    "    # tokenization\n",
    "    X_list = word_tokenize(X) \n",
    "    Y_list = word_tokenize(Y)\n",
    "\n",
    "    # sw contains the list of stopwords\n",
    "    sw = stopwords.words('english') \n",
    "    l1 =[];l2 =[]\n",
    "\n",
    "    # remove stop words from the string\n",
    "    X_set = {w for w in X_list if not w in sw} \n",
    "    Y_set = {w for w in Y_list if not w in sw}\n",
    "\n",
    "    # form a set containing keywords of both strings \n",
    "    rvector = X_set.union(Y_set) \n",
    "    for w in rvector:\n",
    "        if w in X_set: l1.append(1) # create a vector\n",
    "        else: l1.append(0)\n",
    "        if w in Y_set: l2.append(1)\n",
    "        else: l2.append(0)\n",
    "    c = 0\n",
    "\n",
    "    # cosine formula \n",
    "    for i in range(len(rvector)):\n",
    "            c+= l1[i]*l2[i]\n",
    "    \n",
    "    try:\n",
    "        cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "    except:\n",
    "        cosine = 0\n",
    "    \n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program to measure the similarity between \n",
    "# two sentences using cosine similarity.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df = pd.read_csv('Final.csv')\n",
    "\n",
    "cs_simStory = []\n",
    "cs_simRisk = []\n",
    "\n",
    "for i,row in df.iterrows():\n",
    "    X = df.story_urlA.iloc[i]\n",
    "    Y = df.story_urlB.iloc[i]\n",
    "    cosValS = cos_Sim(str(X), str(Y))\n",
    "    cs_simStory.append(cosValS)\n",
    "    \n",
    "    I = df.risk_urlA.iloc[i]\n",
    "    J = df.risk_urlB.iloc[i]\n",
    "    cosValR = cos_Sim(str(I), str(J))\n",
    "    cs_simRisk.append(cosValR)\n",
    "#     print(\"similarity: \", cosValS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cs_simStory'] = pd.DataFrame(cs_simStory)\n",
    "df['cs_simRisk'] = pd.DataFrame(cs_simRisk)\n",
    "\n",
    "df.to_csv('CS_Output.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
